""" DenseNET Melanoma:
	Pixel=32, Depth=200, time/epoch = 120s, parameters 2,587,220, non-trainable = 80,532, test_accuracy=0.82667
	Pixel=64, Depth=100, time/epoch = 190s, parameters 2,591,288, non-trainable = 80,532, test_accuracy=0.84
	Pixel=64, Depth=200,  time/epoch = 199, parameters 2,591,288, non-trainable=80,532, test_accuracy=0.85500
	Needed Large computing power for Pixel=128
"""

from __future__ import print_function
import keras
from keras.callbacks import ModelCheckpoint, LearningRateScheduler
from keras.callbacks import ReduceLROnPlateau, CSVLogger
from keras.preprocessing.image import ImageDataGenerator
from keras.regularizers import l2
from keras import backend as K
from keras.optimizers import RMSprop
from keras.models import Model

import numpy as np
import pickle
import os
from DenseNET_model import DenseNET

w = 64 # The image pixel used for datageneration

batch_size = 4 
epochs = 200
data_augmentation = True
num_classes = 2
num_dense_blocks = 3
use_max_pool = False
subtract_pixel_mean = True
n = 37

growth_rate = 12
depth = 200
num_bottleneck_layers = (depth - 4) // (2 * num_dense_blocks)

num_filters_bef_dense_block = 2 * growth_rate
compression_factor = 0.5
#n = int(sys.argv[1])


model_type = 'DenseNET%d' % (depth)



train = pickle.load( open( 'ISIC2017_train_melanoma_colored_'+str(w)+'.p', "rb" ) )
val = pickle.load( open( 'ISIC2017_val_melanoma_colored_'+str(w)+'.p', "rb" ) )
test = pickle.load( open( 'ISIC2017_test_melanoma_colored_'+str(w)+'.p', "rb" ) )


x_train = train['features']
y_train = train['labels']
x_val = val['features']
y_val= val['labels']
x_test = test['features']
y_test = test['labels']

# Input image dimensions.
input_shape = x_train.shape[1:]

# Normalize data.
x_train = x_train.astype('float32') / 255
x_val = x_val.astype('float32') / 255
x_test = x_test.astype('float32') / 255

# If subtract pixel mean is enabled
if subtract_pixel_mean:
    x_train_mean = np.mean(x_train, axis=0)
    x_train -= x_train_mean
    x_val -= x_train_mean
    x_test -= x_train_mean
    
# Convert class vectors to binary class matrices.
y_train = keras.utils.to_categorical(y_train, num_classes)
y_val = keras.utils.to_categorical(y_val, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)
# Input image dimensions.

def lr_schedule(epoch):
    lr = 1e-3
    if epoch > 180:
        lr *= 1e-4
    elif epoch > 160:
        lr *= 0.5e-3
    elif epoch > 140:
        lr *= 1e-3
    elif epoch > 120:
        lr *= 0.5e-2
    elif epoch > 80:
        lr *= 1e-2
    elif epoch >60:
        lr *= 0.5e-1
    elif epoch > 0:
        lr *= 1e-1
    print('Learning rate: ', lr)
    return lr


model = DenseNET(input_shape, depth, num_classes, num_dense_blocks, growth_rate, compression_factor, use_max_pool, data_augmentation)
model.compile(loss='categorical_crossentropy',
              optimizer=RMSprop(1e-3),
              metrics=['accuracy'])
model.summary()

# Prepare model model saving directory.
save_dir = os.path.join(os.getcwd(), 'saved_models')
model_na='IS17_DenseNET_melanoma_'+str(depth)+'_'+str(w)+'p2'
model_name = model_na+'.h5'

if not os.path.isdir(save_dir):
    os.makedirs(save_dir)
filepath = os.path.join(save_dir, model_name)

# Prepare callbacks for model saving and for learning rate adjustment.
checkpoint = ModelCheckpoint(filepath=filepath,
                             monitor='val_acc',
                             verbose=1,
                             save_best_only=True,
                             save_weights_only=True)

lr_scheduler = LearningRateScheduler(lr_schedule)

lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),
                               cooldown=0,
                               patience=5,
                               min_lr=0.5e-6)

csv_logger = CSVLogger(model_na+'.log')

callbacks = [checkpoint, lr_reducer, lr_scheduler, csv_logger]

# run training, with or without data augmentation
if not data_augmentation:
    print('Not using data augmentation.')
    model.fit(x_train, y_train,
              batch_size=batch_size,
              epochs=epochs,
              validation_data=(x_val, y_val),
              shuffle=True,
              callbacks=callbacks)
else:
    print('Using real-time data augmentation.')
    # preprocessing  and realtime data augmentation
    datagen = ImageDataGenerator(
        featurewise_center=False,  
        samplewise_center=False, 
        featurewise_std_normalization=False, 
        samplewise_std_normalization=False,  
        zca_whitening=False, 
        rotation_range=0,
        width_shift_range=0.1,
        height_shift_range=0.1,
        horizontal_flip=True,
        vertical_flip=False)

    datagen.fit(x_train)

    # fit the model on the batches generated by datagen.flow()
    model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),
                        steps_per_epoch=x_train.shape[0] // batch_size,
                        validation_data=(x_test, y_test),
                        epochs=epochs, verbose=1, workers=4,
                        callbacks=callbacks)

# Loads best weight, if you just want to the prediction and evaluation of data
model.load_weights('saved_models/'+model_name)
model.compile(loss='categorical_crossentropy',
              optimizer=Adam(lr=lr_schedule(0)),
              metrics=['accuracy'])

scores = model.evaluate(x_test, y_test, verbose=1)
print('Test loss:', scores[0])
print('Test accuracy:', scores[1])